# 整体概述  
## 是什么——图像/视频质量评估是在做什么事  
其实概念比较好理解没啥复杂的，字面意思就是评价图像/视频的质量，给出一个量化的分数  
视频就是VQA Video Quality Assessment，图像就是IQA Image Quality Assessment，现在还会做点云质量评估PCQA Point Cloud Quality Assessment  
但是难在这个“质量”是比较抽象的，一般不太指审美的评分（这个更难），而是所谓的“画质”好不好。这个评估生活中更多的是主观感受难以量化，所以理解这个任务是做什么，先要理解主观&客观VQA/IQA的定义  
### 背景补充——主观&客观质量评估  
人主观评价就是主观质量评估，不同人的评估标准都不统一，是非常难量化的。目前研究更多依赖大量人主观评价的数据库的统计数据作为标准，算是一个统计上合理的量化标准  
计算机通过数学模型给出量化评价就是客观质量评估  
做QA方向一个根本的目标就是自动生成和主观质量评估一致的客观质量评估，说人话就是让机器打分和人打分差不多  
## 为什么——QA的意义  
为什么要让机器和人打分差不多？因为有一些活就是需要机器来做的，乙方（机器）做事情不可能一遍一遍去问甲方（人）的标准，而标准不一就会导致乙方做的甲方不满意又打回重做，所以尽量一开始就统一标准。具体来说有以下一些场景需要VQA/IQA  
* 指导图像和视频编码  
有损压缩要尽量少损失画质，那怎么评价画质的损失？就需要用QA的量化指标  
* 指导图像处理算法  
怎么评价一些图像去雾、去噪算法做的好不好，也需要有量化的指标  
* 评估视频相关应用服务质量  
这里主要是VQA了，网络传输、设备故障造成的一些视频卡顿、花屏问题也需要通过VQA检测出来  
  
## IQA问题分类  
* 全参考 Full Reference-IQA, FR-IQA  
直接有原始（无失真）图像可以作为参考对比，很好做而且研究成熟  
  
* 半参考 Reduced Reference-IQA, RR-IQA  
只有原始图像的部分信息/提取的特征，介于FR和NR之间。定义有一点模糊，应该对图像做了变换或者特征提取啥的也能叫做RR，相关研究少一些。  
* **无参考/盲参考 No Reference-IQA/Blind IQA, NR-IQR/BIQA**  
最难做，但是也最有研究意义。  
还可以分为特定失真类型质量评估（例如针对模糊/块效应/噪声等失真的）和通用的质量评估。  
  
## VQA和IQA区别  
VQA以IQA为基础，但除此之外还引入更多需要考虑的。  
TODO  
  
# 具体做法  
## 关于评估指标  
### 主观评估常用指标  
MOS Mean Opinion Score 平均主观得分，就是一个量化的得分，根据大量人主观评分测试统计而来，不同数据集的得分取值范围可能不统一，还需要做映射等处理  
DMOS Differential Mean Opinion Score 平均主观得分差异，就是MOS的标准差/方程，看看主观评分争议大不大  
### 主观客观差异常用指标  
评价客观的QA算法好坏，就是要和主观评估的分数对照，这里也需要有量化的指标  
* LCC/PLCC  
Pearson Linear Correlation Coefficient，皮尔森线性相关系数，预测IQA算法准确性，结果越接近1边分数越接近  
$PLCC=\frac{\sum_{i=1}^N(y_i-\bar{y})(\hat{y_i}-\bar{\hat{y}})}{\sqrt{\sum_{i=1}^N(y_i-\bar{y}^2)}\sqrt{\sum_{i=1}^N(y_i-\bar{\hat{y}}^2)}}$  
* SRCC/SROCC和KRCC/KROCC  
Spearman rank-order correlation coefficient 斯皮尔曼秩相关系数  
$SRCC=1-\frac{6\sum_{i=1}^N(v_i-p_i)^2}{N(N^2-1)}$  
Kendall rank-order correlation coefficient 肯德尔秩相关系数  
预测IQA算法单调性，即预测评分结果的排名是不是和主观评估结果的排名一致，按顺序从1到结尾的，完全符合为1，完全倒序为-1，乱序的根据程度在[-1,1]之间  
### 全参考常用指标  
* PSNR  
Peak-to-Peak singal-to-noise ratio 峰值信噪比  
最经典的指标，计算简单非常高效  
基本的思路就是计算原始图像和失真图像之间灰度差异的均方误差MSE，但是这个MSE就是一个绝对的标量难以评判是大还是小，那就用最大像素值平方（一般255，即信息）和MSE（即噪声）做比然后取log得到信噪比。越大越好，一般认为低于30dB就失真比较严重  
$PSNR=10\log_{10}\frac{255^2}{MSE}$  
* SSIM  
Structural Similarity Index 结构相似性  
滑动窗口去对比原始图像和失真图像的亮度&对比度&结构差异，因为不只是考虑了像素点差异，对于压缩失真等像素差距小但结构上差距大的，一般能比PSNR取得更好的效果  
其他还衍生出一些优化版本，例如MS-SSIM（多尺度缩放后做SSIM），更大的计算开销可以获得更好的效果。  
* VMAF  
是Netflix提出的一个开源的全参考视频质量评估工具，给出一个0-100的质量评分。  
原理是先提取了三个视频质量指标：视觉信息保真度（VIF。这里我查到并不是直接比的原始图像和失真图像，而是一起经过一个HVS模型模拟人眼视觉感受之后再比较失真，会更符合人眼的感觉）、加性失真测量（ADM。这里我查到有小波变换啥的去分析细节的加性失真）、运动特征（Motion。大概是不只参考静帧，还要结合运动特征分析帧间预测的参考帧）。三种指标可以并行分开算提升速度，不过其中VIF一般最耗时。  
提取之后使用SVM支持向量机去融合指标，输出一个VMAF分数。这样就考虑了比较全面的质量信息，虽然是一个客观指标，但更接近人眼的感受，实际应用中效果还不错。不过由于步骤复杂，VMAF更像是一个“黑盒”指标，不好简单地描述其特性。  
* (AVQT)  
这里只是一起记录下，这个是apple 2021 WWDC上发布的一个全参考视频质量评估工具Advanced Video Quality Tool。效果还不错，有的论文里会和VMAF一起作为对比的指标。但是这是闭源的，原理未知。  
  
## 关于数据集  
### IQA数据集  
比较经典的4个常用的IQA数据集列在表里  
| 数据集 | 参考图像数 | 失真图像数 | 失真类型数 | 测试人员数 |  
| --- | --- | --- | --- | --- |  
| TID 2013 | 25 | 3000 | 24 | 971 |  
| TID 2008 | 25 | 1700 | 17 | 838 |  
| CSIQ | 30 | 866 | 6 | 35 |  
| LIVE | 29 | 779 | 5 | 161 |  
还有一些时间久的IVC、Toyama、A57、WIQ数据量比较小了  
TODO: 也有一些新的可能还没有完全普及作为通用标准，需要研究下，例如KonIQ-10K, PieAPP, PIPAL, FLIVE, SPAQ, KADID-10k  
#### TID 2013  
这个是目前最常用最权威的数据集。都是512\*384的BMP文件。3000张=25张图片\*24种失真类型\*5个失真等级  
TODO: 具体格式  
### VQA数据集  
* CVD2014  
2014年赫尔辛基大学做的，算偏早期的数据集了。  
78 个不同的相机（手机、小型相机、摄像机、单反相机）拍摄的来自五个不同场景的 234 个视频组成  
主观评分这里好像分了组做实验，这里没看明白，可能不太容易直接拿来用。有6G、16G、23G三组数据。  
* LIVE-Qualcomm  
2017年UT Austin做的，8 种不同移动设备拍的208个视频，54个场景，一个视频15s，都是1080p的，模拟了六种常见的拍摄失真类别。  
每个视频都由 39 个不同的受试者进行评估  
* KoVNiD-1k  
2018年，1200个视频，一个8s  
质量评分是1-5五个等级。  
大概2.8G，非常小，一个视频就1M左右。  
* LIVE-VQC  
2019年UT Autstin做的，585个视频，80 个不同的用户使用 101 个不同的设备（43 个设备型号）拍摄，分辨率帧率不统一，算是比较真实丰富的。  
众包搞了205000 个意见评分，平均每个视频有 240 个人工评价，评分是0-100。  
大概5.5G，倒是比较小。  
* Youtube-UGC  
2020年Youtube提供的一个精选UGC数据集，大约1500个视频每个20s，UGC的视频包含各种类型，很强的一点是每个视频都支持了多种分辨率。  
每个视频都有100+的主观评分，是1-5的值。还人工打了内容类型标签。  
官网上有下载链接不知道能不能下，YUV的格式2T、H.264 110G、VP9的只有20G。  
* LSVQ  
2020年还是UT Austin做的一个目前数量最大的VQA数据集，真的好有钱。包含39,000 个现实世界的失真视频和 117,000 个时空局部视频补丁（“v-patches”，TODO：这是什么？）。  
有 5.5M 人类感知质量注释。  
原论文Github上有下载工具，听说可能下不全。自己试了下还真是有问题，分了两部分，第一部分是给了colab脚本去下载，速度非常慢……第二部分还是box网盘填写信息密码发邮箱，但是收不到密码……Hugging Face上teo wu看传了一份，大概不到100G可以下。  
* MSU-VQMB  
就36个视频，不过都是比较高清的。  
总共收集了来自 10,800 个参与者的 766,362 个有效评价。  
* DIVIDE-3k  
2022年Weisi Lin老师团队考虑审美分数新建的数据集，3634个训练视频，909个验证视频。  
每个视频都有审美分数、技术分数和整体分数  
TODO  
### PCQA数据集  
WPC、SJTU-PQA、M-PCCD、IRPC  
SIAT-PCQD、PointXR，一般点云质量评估是固定观察距离的图片/可旋转视角去评估，这两个是支持6DoF移动观察的  
#### WPC  
Waterloo Point Cloud，滑铁卢大学发的一个比较大的彩色点云质量评估数据集，对应论文Perceptual Quality Assessment of Colored 3D Point Clouds（2021）  
  
# 一些研究的思路  
## 特征工程  
### 预处理（采样）  
在VQA任务的数据预处理中，采样是核心，可以降低处理数据开销、匹配模型输入size、提升算法性能  
#### 空域  
* crop  
分为随机、中心、指定点等等  
* resize  
一般是缩小的resize，这也是一种下采样，涉及到不同算法如最近邻、双线性、Area、Lanczos等等  
* 网格采样拼接  
FastVQA的Grid Mini-patch Sampling（GMS）方法，分网格后在网格内部随机采样然后再拼接成一块  
#### 时域  
* 随机采样  
* 均匀采样  
* 关键帧  
需要额外获得关键帧信息，场景变换一定有关键帧可能会处理的好一些  
* 平均分段后段内随机起始位置等间隔采样（可重叠）  
首先是经典TSN网络提出的分段clip后随机采样一帧方法，简单有效。后面又升级了下，在一个分段内随机起始位置按间隔连续采样n帧。  
这个算是比较通用的做法了，可以记为clip_len x frame_interval x num_clips，即每一段采几帧x间隔几帧采一帧x总共分几段。具体实现参考openmmlab的mmaction2中SampleFrames。注意是允许不同片段采样帧重叠的，例如30帧，clip_len=5，frame_interval=2，num_clips=3，做法不是分成不重叠10x3的三段，然后内部抽5帧，实际实现是(30-5x2+1)/3=7，得到分段起始点为[0,7,14]，然后每一段0-6随机偏移，例如变成[2,10,18]，接着顺着间隔2帧采连续5帧。  
* 平均分段后段内随机起始位置等间隔采样（不可重叠）  
感觉这个想法更直接一些，分段然后每个段里取自己的。这也是Faster-VQA的做法，说相当于时域的网格采样，还挺形象，但感觉实际和上面可重叠的做法没太大区别，减少一点采样数不是一样的吗？  
#### 进阶思路  
##### 多尺度  
空域中低分辨率输入拿全局信息，高分辨率原始输入获得局部纹理细节信息  
##### 时空数据融合  
例如TSM  
### 分析特征权重（ROI/显著性检测）  
模拟人类视觉更关注图片重点内容，显著的区域质量重点处理。ROI (Region of Interest, 感兴趣区域)  
可能需要结合ROI、显著性检测、边缘检测等方法  
最终的效果就是拿到了图像某一区域的权重信息（可能二值也可能多级），有了这个信息就可以做分层采样、分区域评分增加权重等等操作  
#### 时域重要片段检测  
和空域同理，时域上也会有重要的视频片段和不重要的片段。在视频理解等任务中可能比较明显，但是VQA任务中按说没有那么突出，可以作为一个考量。  
可能需要结合动作识别、场景识别等模型。  
如果拿到不同时域片段的权重信息，也可以时域分层采样、分片段设置评分权重等  
### 特征融合  
不同来源不同层次的特征搞在一起。手动搞得特征很复杂的时候就需要想办法怎么合在一起送入网络。  
#### 早期融合  
直接送入Transformer的话基本上就是想办法都搞成Token拼接在一起，或者加和在一起（如位置编码）  
#### 中期融合  
在中间层合并一些特征，这里能做的比较复杂，例如一些旁路模型去获取权重信息去对中间层特征做加权  
#### 后期融合  
这个也可以说是多模型集成学习，即多个模型都给出结果了再去得到一个最终结果。  
### 结合传统特征  
* 频域信息  
* 直方图  
## 模型魔改  
### 新模型  
#### Mamba  
#### RMKV  
### 多模型融合  
#### 集成学习  
多种各有优势的网络模型合在一起，利用各自模型的优势场景分开处理不同的输入，获得的结果可以再通过简单的投票选一个/加权平均、复杂的MLP网络等方法，得到最终结果  
模型可以分开训练。但是推理、部署的复杂度还是会提高。  
#### 混合专家MoE  
通过门控机制来根据情况选择最适合的模型  
动态选择模型，训练会比较难做  
### 压缩模型  
#### 知识蒸馏  
#### 量化  
混合精度训练AMP，已经做了。据说CV任务中bf16可以了，再小会有问题  
后量化  
#### 裁剪  
CNN好做一些，看权重低的连接、filter、通道就直接移除了。Transformer中可能也类似，多头去点头、减少层级、去除权重低的连接。  
放到Mamba里面感觉不敢动了，原理也都理不太清……  
## 数据集问题  
### 主观评分  
主观评分很难搞，找人评图像/视频建立数据集还是成本很高的事情，而且还需要按标准才能得到比较靠谱的主观评分  
#### LLM  
结合LLM多模态输入图像给出文本评价的能力，进行主观评分  
### 数据增强  
感性的结论是VQA任务不应该做数据增强。经典的一些方法例如加噪声、变颜色、Mixup等方法应该都是不能用的，对画面质量会有影响。  
一些空间变换如翻转、裁剪、resize理论上可能有效果。但负面影响未知。  
视频画质增强、生成模型绘制等方式明显也会改变质量评分。  
那有没有能用的？用了之后效果如何？其实是值得分析的问题。  
#### JND  
最小可察觉误差JND，把来自人眼视觉系统特性&心理效应的视觉冗余量化出来，可以指导编码和质量评估。通过JND来生成一些评分接近的训练数据做增强  
#### 量化相对得分  
分析resize、crop、sharpen等一些图像处理操作对图像质量评分的相对影响，在对原数据进行一些操作进行数据增强时，可以根据相对得分计算出一些假的分值，或许可以有用  
这个是不是和zero-shot相关，通过数据的关系去推测标签的关系？  
### 小数据集  
#### 迁移训练  
大数据集LSVQ上预训练的再放到小数据集上微调  
#### 多任务学习  
用数据量多的视频理解等任务预训练的模型作为底层模型，提取出公共的特征  
#### 动态调整模型  
VQA任务的主观性很强，如果不同用户有不同的评判标准（尤其是审美方面），已训练好的数据，能否实现用户自己个性化微调的效果？  
在线学习？增量学习？  
  
## 参考其他任务  
### 和视频理解任务关联  
视频质量评估任务和视频理解任务相对而言是相似程度比较大的，都需要对视频空域时域的信息进行分析。而相比之下，视频理解任务是一个更加热门的研究领域，实际中很多VQA任务模型也会用到从视频列  
  
# 论文整理  
## VQA  
有很多早期的模型现在看来指标都比较差了，所以没有往前看很多，可能会漏掉一些有意义的idea，有机会再看吧。整体看近年的论文大都是在采样方法上做文章，还是有很多相通之处的。  
### （19.10.5北大 VSFA）  
问题：VQA有时间滞后效应，人会记住历史的低质量帧然后降低对后续帧的评分，需要考虑长期依赖关系  
方法：先使用CNN提取每一帧的内容特征，每一帧的特征按时间顺序输入到GRU中，使用GRU来解决对历史帧记忆的问题。最终的评分还经过了各个时刻GRU输出结果做池化  
想法：这个处理时间滞后效应问题的想法挺有意思的，给出了用RNN模型的理由。但是从现在的视角看这个问题似乎也没有那么重要，毕竟这里早期模型指标跑出来挺一般的，有点硬加了下GRU的感觉，现在应该很多ViT的模型有多帧信息都是可以处理一点长期依赖的  
### （20.11.27德州大学）Patch-VQ 'Patching Up' the Video Quality Problem  
就是带了LSVQ数据集的经典早期文章。同时做了个PVQ模型，应该说是稍微粗糙的一个经典baseline，基本上后续模型对比都会看到。  
文章中有个很重要的结论，就是视频global和local的主观评分是比较相关的，算是空域&时域采样的理论基础了。具体分析方式是做数据集的时候，搞了3w+的原始视频，又使用非常初级的三种空域时域采样方法得到了3倍的patch，分别MOS评分判断相关性。采样方法有：空域采样（时域不采样，空域随机crop 16%面积，SRCC=0.69）；时域采样（空域不采样，时间随机连续的40%，SRCC=0.77）；时空域采样（前面的空域采样+时域采样一起做，SRCC=0.67）  
  
一 关于数据集  
强调了UGC视频有各种时空域的短暂失真，包括丢帧、焦点变化、传输故障，而现有UGC数据集太小覆盖不了多少情况。所以做了一个目前最大的UGC数据集。  
二 关于PVQ模型  
这个模型的结构看起来略显复杂，但实际上和后续的SimpleVQA基本是一致的，只是当时还没有那么清晰的思路说一路输入2D帧取空域内容&纹理特征，一路输入低清3D取时域动作特征。  
具体到细节上是用PaQ2PiQ网络提取2D的视频特征，ResNet3D网络提取3D的视频特征，然后做了ROI和SOI的池化（这个效果如何值得分析下），最后用一个InceptionTime时间序列回归模型来给回归得分。  
  
想法：SimpleVQA对比这个，差不多就是升级了2D和3D视频特征提取网络的Backbone，改为了，池化和回归好像都做的更加简单一些，我觉得是一个非常好的结构设计。  
### （21.6.22港城大 GST-VQA） Learning Generalized Spatial-Temporal Deep Feature Representation for No-Reference Video Quality Assessment  
这个好像后面模型对比的少一些，主要的思路是想提取多尺度特征，从而可以适配不同分辨率，不同时长的视频输入。  
我觉得比较特别的点是网络结构设计的好像稍有点复杂，我有点没太看明白，大概是VGG16->Transformer->MLP->GRU，认为这样子有空域的多尺度特征，然后对于GRU输出的多帧时域结果，训练阶段还做了高斯正则化，测试时还做了金字塔特征聚合，可能对短期长期的时域影响处理的更好。  
因为最后指标跑出来一般般，这个模型不细分析了，往多尺度的思路去思考还是可以的。  
  
### （21.8.19上交 BVQA）Blindly Assess Quality of In-the-Wild Videos via Quality-Aware Pre-Training and Motion Perception  
SimpleVQA的前身，大体思路是相似的。  
还得到了结论是在IQA数据集和动作识别数据集上预训练的模型迁移到VQA任务中是有效的。  
### （21.8广州大学）StarVQA Space-Time Attention for Video Quality Assessment  
算比较早用纯ViT的模型，创新性上倒是没啥特别的地方。看是直接随机crop224x224然后输入ViT了，指标跑出来在大数据集LSVQ上挺好的，小数据集上就比较差，毕竟ViT。  
如果需要对比ViT Backbone的话可以看下这个代码。  
### （21.10挪威研究中心 LSCT-PHIQ）Long Short-term Convolutional Transformer for No-Reference VQA  
用了两个看起来结构很复杂的网络，跑了KoNViD和Youtube UGC指标挺不错的。TODO  
先是PHIQNet，是一种考虑了多尺度的CNN，提取出考虑多尺度的特征。  
然后是Long short-term convolutional transformer，长短期卷积Transfomer？  
  
### （22.6.6 NTU）FAST-VQA: Efficient End-to-end Video Quality Assessment with Fragment Sampling  
问题：主要解决了VQA中高清视频数据量过大，计算开销高的问题  
方法：  
* 网格化补丁采样  
把原始视频帧按网格分块，然后每一块里原始分辨率采样一个小补丁，不同帧都在相同位置采样这个小补丁。——这样就采样获取了最细节局部纹理信息  
把所有网格的小补丁拼接在一起组成一个“大补丁”作为输入。——这样就有了个很抽象的全局语义信息（直观感觉有点弱，这都成块了啥也看不出来）  
* FANet网络  
backbone用的是4层注意力层的Swin-T网络，但还要做一点调整。主要问题是大补丁毕竟是硬拼起来，小补丁内Intra-patch像素关联强，小补丁之间cross-patch像素关联弱。所以，一是做注意力计算的时候要有所区分，加一个偏差值隔开不同小补丁；二是不能先池化再非线性回归（这是激活函数吗？）会把小补丁混了，顺序改成先非线性回归再池化。  
  
想法：这样很小的采样又硬拼在一起，相比更早的方法直观感觉是增加了细节纹理的权重、削弱了全局语义的权重，也能取得很好的效果，这一点是挺令人惊讶的，应该说明了目前的质量评分基本上还是以局部细节纹理为主吧。  
但是由于随机采样引入了评分结果不稳定的问题，只有在整体数据集上统计表现不错，但对于具体单个视频误差较大。  
  
22.10的Faster-VQA又在时域采样上搞了点操作说是时域的网格采样，但是个人觉得没什么意思，和普通的时域采样没明显区别。  
  
### （22.6.20 NTU） DisCoVQA: Temporal Distortion-Content Transformers for Video Quality Assessment  
问题： 时域失真（shaking、 flicker、abrupt scene transitions）如果仅仅是固定时域采样的话难检测，需要重点关注时域失真的VQA模型  
方法：  
通过一个Transfomer提取时域失真（TODO：怎么实现的？是每一帧都输入吗）并获取每一帧的质量评分，这个Transfomer的输出再输入到另一个Transfomer进行内容的分析给出每一帧的质量权重，加权得到最终评分。  
这样的做法明显就是给了时域质量更多的权重，看起来指标一般吧，可能还是需要时域失真比较明显的数据集才能有明显优势。  
  
### （22.10.9广州大学 HVS-5M）HVS Revisited: A Comprehensive Video Quality Assessment Framework  
拼了5个模块提取5种特征输出最后的总分，指标也还不错，这几个特征可以关注下，也是比较直接的加人工特征的方式。TODO  
用的Motion Perception、Temporal hysteresis、Content dependency、Visual saliency、Edge masking  
  
### （22.10.20上交 SimpleVQA）A DeepLearning based No-reference Quality Assessment Model for UGCVideos  
和FastVQA一样也是目前VQA的经典模型，值得学习。  
核心点是在预处理采样的时候，分了两个路径，一个是高清少帧数的一路看细节纹理，一个是低清高帧数的一路看动作内容。  
空域采样就是采少量分辨率较高的帧520x520再中心crop到448x448，送入ResNet50提取空域特征,。  
时域采样是这个片段全部帧但是resize到低的分辨率224x224，送入SlowFast提取时域信息（应该是动作识别一类的结果）。最终两部分特征再合起来经过一个MLP得到最终评分。  
  
### （22.11.9 NTU DOVER）Exploring Video Quality Assessment on User Generated Contents from Aesthetic And Technical Perspectives  
问题：之前的VQA只关注了技术角度的质量评分  
方法：  
* 做了DIVIDE-3k数据集  
每个视频标签是技术评分+审美评分+整体评分  
* 提出了DOVER视频质量评估模型  
就是分了两路分别计算技术评分和审美评分  
TODO：DOVER具体网络设计，我理解技术分应该还是类似Fast-VQA的处理，审美分主要依赖全局语义可以做下采样，但是提了个Cross-scale Regularization没看懂是什么，不同尺度下采样之间比较吗？  
  
### （23.4 NTIRE VQA竞赛）NTIRE 2023 QA of Video Enhancement Challenge  
淘宝团队基于SimpleVQA的模型拿了冠军，在数据增强上做了比较多。TODO：需要借鉴下  
### （23.4.19上交）MD-VQA Multi-Dimensional Quality Assessment for UGC Live Videos  
TODO  
  
### （23.5.22 NTU MaxVQA）Towards Explainable In-the-Wild Video Quality Assessment: A Database and a Language-Prompted Approach  
基于DOVER时搞的DIVIDE-3k数据集，又扩展了评分的维度，本来只有审美评分+技术评分+整体分，又对数据集里4543个视频收集了200w+的文本评价，把这些评价转换成了13个维度的评分。这个数据集确实是个极大的亮点，借助文本语义的信息来解决原本视频质量只有技术评分的问题。  
提出了MaxVQA，这里的Max指的是多维度。  
TODO：网络设计需要再看下，技术分还是用了Fast-VQA，其他语义评分的处理用了做多模态的CLIP  
### （23.7快手 VQT） Capturing Co-existing Distortions in  User-Generated Content for No-reference Video  Quality Assessment  
TODO  
  
### （23.12.28 NTU）Q-ALIGN: Teaching LMMs for Visual Scoring via Discrete Text-Defined Levels  
使用多模态大模型，输入是图像和文字问题（评分如何？），输出是5个主观评分级别。训练后可以实现利用大模型来做主观评分，并且同时支持图像质量、图像美学、视频质量的评分。  
### （24.2.20快手 KSVQE）KVQ:KwaiVideo Quality Assessment for Short-form Videos  
应该是目前的SOTA，在FastVQA网格采样的基础上，又加了两个旁路网络，一个是获得不同分块的权重（似乎还有内容识别），一个是检查一些形变失真（这个听起来没啥意思）。  
TODO：这个要认真分析下。  
  
### （24.2.29港城大）Modular Blind Video Quality Assessment  
CVPR 2024  
还是个集成3个模型的VQA模型～  
目前的VQA模型比较难处理的一个问题就是整个视频输入进去太大了没办法处理，无论原视频多大，固定时域采样只取8/16/32帧，空域采样每一帧裁剪/网格采样到224x224，这样就会丢时域和空域的信息，一些失真就发现不了（但从公开数据集上的跑分看，采样很少依然效果不错）。按说视频分辨率帧率越高采样损失就越多，所以很多模型就会提一些方法来找补。这里是：  
1. 还是用个普通VQA模型，时域空域都采样（文中的基础质量预测器）  
2. 加一个空域不采样、时域采样的VQA模型，也不是输入全图，是算了全图的拉普拉斯金字塔做特征（文中的空间整流器）  
3. 加一个时域不采样、空域采样的的VQA模型，可以多输入一些连续帧了（文中的时间整流器）  
最后拿2、3模型的输出矫正1模型的输出，思路挺清楚的，理论上能发现一些别的模型发现不了的问题，就是在大部分公开数据集上刷分看提升不大，在4K、120Hz这种高分辨率高帧率的数据集上效果好，但复杂度估计也挺高  
### （24.4.17快手组织竞赛论文）NTIRE 2024 ChallengeonShort-form UGC Video Quality Assessment: Methods and Results  
可以看到很多魔改模型的思路去借鉴。因为卷的是准确率，所以基本上都是多模型在一通拼，魔改的会很复杂。  
第一名是上交SimpleVQA作者和NTU FastVQA作者的联队，这有点欺负人了，直接把SimpleVQA和FastVQA的结果组合了，还附加了Q-Align和LIQE两个模型的结果一起，4个拼在一块很强。  
但是整体看下来没有什么新的东西，毕竟是刷分的比赛，基本都是流行的这几个主流模型去一通合并，不过也可以看出来FastVQA（含Dover）、SimpleVQA、Q-Align等几个模型是经过检验的模型。  
### （24.5快手 PTM-VQA） PTM-VQA: Efficient Video Quality Assessment Leverage Diverse PreTrained Models from the Wild  
TODO