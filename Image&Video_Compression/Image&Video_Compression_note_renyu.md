# 视频编码基础  
  ## 基本原理  
  压缩/编码是什么？减少信息的bit数，使其适合storage or network  
  压缩系统处理流程很简单，是encode-存储/传输-decode  
  在通信系统中看压缩做的是信源编码，压缩冗余信息，方便存储和传输。区分信道编码用于纠错，避免传输过程噪声干扰导致错误  
  还分了有损压缩和无损压缩，这个概念很简单，就是看能否恢复一模一样的。  
  视频编码一般是有损压缩。评价一个有损的压缩算法，一方面看压缩比是压缩前bit数除以压缩后bit数，一般是大于1的，一方面看压缩后质量，失真测量Distortion measure/metrics，常用MSE、SNR、PSNR。实际场景中是要做在其中取平衡的，这是一个RDO 率失真优化的问题，意思就是考虑损失的时候同时考虑比特率和失真情况，不是单一为了尽可能压缩比特率或者尽可能减少失真，而是要么在给定失真约束下最小化码率，要么在给定码率约束下最小化失真。  
  ### 图像&视频压缩的依据  
  一是统计冗余，分为：  
    
  * 空域冗余：图像中有很多平坦区域，变化缓慢，相邻像素更为相似  
  * 时域冗余：视频连续帧之间相似  
  * 编码冗余：根据信息论，编码可以将数据大小最多压缩到熵  
    
  二是心理和视觉的冗余（高频部分不敏感(直流和低频分量表示轮廓色块，高频分量代表纹理)、色度不敏感）  
  ### 色度下采样Chroma Subsampling  
  人对明度更敏感，色度差一点。这样就有了压缩的可能。  
  色彩空间有很多种不同的表示方式，例如RGB，但视频编码中使用YUV，或者叫YCbCr。Y分量对应明度不能少，但UV分量对应色度可以采样  
  不同采样级别的表示方式就是444 422 420 411那个。444 4x2的块，YUV都是4x2的块不下采样，422是UV都水平2:1采样只留2x2的块了，UV都变成原来1/2的数据量，整体变3/2，420就是水平竖直都2:1采样，只留2x1的块，UV都变成原来1/4的数据量，整体变1/2。  
  ### 熵编码Entropy Encoding  
  补充下信息论关键概念：信息的熵η是按symbol的出现概率取导数再取对数。熵可以认为是信息大小的下限，只能接近但不可能低于。  
  熵编码分为定长编码和可变长编码，可变长编码一般是更好的，具体方法比较经典的就是霍夫曼编码和算术编码。  
  其中霍夫曼编码原理就是高频出现的symbol用更少的码字，从而降低整体平均bit数的。  
  ，核心思想是更大概率的pattern用更短的编码表示。具体手算方法是排序所有pattern的概率，最后俩分配0和1，加一块重新排，还是找最后俩分配0和1，这样组成一个很倾斜的二叉树，顺着读出0和1就是这个pattern的编码。  
  但不符合编码时预期概率分布的话可能越压越大，可以用训练集和测试集不一致的想法来理解。  
  ### 差分/预测编码  
  差分/预测编码是一个东西  
  图像中临近区域像素接近，相似像素之间的差值小于像素的绝对值，意味着记录差值可以用更小的数据量来记录信息。  
  差分编码就适用于连续采样，并且采样数据还相似的。音频、图像、视频都可以。  
  encoder编码的时候，输入是实际值，减去最相似的预测像素的值，输出一个预测误差（也就是差值），存起来。decoder解码的时候，输入的是预测误差，然后找到最相似的预测相似加起来，恢复原来的实际值输出。  
  预测值取整为整数。而且化差分能量集中了，用霍夫曼编码也更省。  
  了解概念即可。  
  ### 变换编码Transform Coding  
  通信原理的通信，把信号从一个域换到另外一个域，或者说把一组基换成另外一组基。这样做的意义是某些域下，数据可能变换成更适合压缩的形式，包括能量集中、去除冗余，就可以舍弃一些不重要的基。（从模式识别的角度来看，这样的压缩也可以理解为降维）  
  变换需要是可逆的。  
  * DTFT 离散时间傅里叶变换  
  只是时域离散，但是频域还是连续的，这种形式并不适合计算机计算。  
  * DFT 离散傅里叶变换  
  时域和频域都是离散的了，理论上是可以计算机计算的。  
  TODO：图像压缩为什么不用DFT？应该是不能能量集中  
  * DCT 离散余弦变换  
  如果DTFT离散时间傅里叶变换被展开的函数是实偶函数，那么就只剩下余弦项，离散化就是余弦变换，算是DFT的特殊情况  
  * KLT变换（或叫做PCA主成分分析）  
  效果最好的变换，但是需要针对每一个图像去计算基。  
  * SVD 奇异值分解  
  #### DCT变换具体实现  
  离散余弦变换。2D-DCT。  
  DCT分解结果的组合可以理解为玩乐高积木，简单点就看4x4块的DCT变换，“频域”分解后总共有16种基本的图像（可以叫basic function，16种你可以想成线性无关基向量数也就是维数），可以查到这个图片学习下，其实还真是对应着不同频率离散波形的感觉，不同分量的16种基本图像的组合就可以得到任意的4x4图像块。左上角的basic function是纯色的，也可以叫做直流分量，越往右下走就是越高频，DCT变换填入4x4方块种的就是这对应16种基本图像的系数，低频分量多能量也会集中在左上角更好压缩。  
  回头再深入理解下变换，你可以理解为换了基，本来基是每个点为1其他为0的basic function，DCT就会换成奇怪的基，KLT也会换奇怪的基，所有的知识都和线性代数串起来了。  
  JPEG用的8x8。  
  ##### 对照公式直接计算  
  目标是把空域图像$S_{ij}$化成频域$S_{uv}$，左上开始，i和u是向下的行数，j和v是向右的列数。TODO：公式……  
  虽然一般用计算机直接DCT，但是可以根据公式练练4x4的DCT手算方便理解，但实际算起来2D每次一搞16个数16x16的计算去整还是计算量有点难搞，可以算的时候分两段做，大致的情况是第一步先按行，$F_[iv}$先固定i找到一行，然后对于每一个v（列）就是按公式累加原矩阵第i行的四个元素。得到F矩阵之后，$S_{uv}$计算是先固定v找到一列，然后对于每一个u（行），就是按公式累加F矩阵第v列的四个元素。所以一共是4个元素x4个元素x2阶段=64次乘法，很麻烦。只是比原来16x16=256次的乘法简单一点点。  
  ##### 矩阵乘法计算  
  更厉害的方式是可以用矩阵乘法来计算DCT结果，即F(u,v)=T\cdot f(i,j) \cdot T^T，T就叫做DCT矩阵，可以用公式求出T，按公式第一行都是固定值，后面就是改cos里的值还听有规律，这样做矩阵乘法可能就直接一点了，而且从DCT逆变换回去也就直接f(i,j)=T^T \cdot F(u,v) \cdot T了，另外由于DCT矩阵是正定矩阵，所以转置矩阵等于逆矩阵。注意u和u是行数，j和v是列数。T矩阵的公式也依照公式给出，对于nxn的矩阵而言T矩阵是固定的，直接拿来用就行，计算量也并不少，只是说适合计算机计算。  
  ##### 用DCT的原因  
  能实现能量集中和去除冗余。更特别的优势是效果好、basic functions是固定的不用每个图去算。当然效果没有KLT那么好。能量集中形象一点看，以4x4分块为例，可以理解为默认的基是16个每个点为1的4x4块，改成DCT变换的16个基会更适合压缩。左上低频，右下高频，能量集中在低频。  
  #### KLT变换具体实现  
  KLT变换。思想也是要利用变换到变换域之后的好特性。是图像压缩的非常重要的一个变换方式，能取得最优效果。理论上是做的旋转数据的坐标系，可以显示出各个数据所在的子空间（有点抽象）。也可以叫做PCA主成分分析或者EVD特征值分解。  
          优势：能取得能量集中的最优解，比DCT还强。去除系数相关性（就是DCT里整的临近像素点之间相关的空域冗余被去除了，DCT变换完还会剩一点冗余，但是KLT是完全去除）。实现降维。  
          缺点：是和图片相关的，不是DCT那样有固定的basis functions（基函数），KLT的basis functions需要根据图像计算。需要估计图像的协方差矩阵，不好算。需要特征值分解也不好算。  
  KLT的核心思想是找到能让数据分布最“分散”的新变换坐标系，初始的直角坐标系就是标准正交的i和j，任意一点做对横轴、纵轴的垂线，交点距离就是坐标值，这都是非常熟悉的初级内容了；而为了能量集中，我们需要找到数据分布最长的一个方向作为横轴，然后垂直的方向作为纵轴，这样重新建立坐标系的好处是横轴分量多了，纵轴分量少了，有损压缩就是扔掉少的部分留下主体部分，完美符合要求。想想扔掉一个轴的信息，那二维平面的数据就会被压到一维的线上，就损失了信息，但是在另一个轴分量很小的情况下，扔掉了影响也不大，减少了压缩的信息损失。  
  那如何计算这个最佳的新坐标系？感性上当然是数据分布最多的为坐标轴，但需要量化。如何确定对于这一堆点更重要的坐标轴？可视化当然能看出来，但是怎么数学计算。简单而言做法就是先挪一下坐标轴，对所有点做中心化zero-mean centering，也就是重新变换坐标轴放到中心位置，这里是与平均向量作差得到的。  
  然后把问题转化为一个求误差最小的投影方向的问题，这里稍微说一下KLT的原理证明：几何上应该就是找到一个新的坐标轴向量$\phi$，所有点都投影到上面，然后希望投影后的结果作为原始数据表示的话是误差最小的，即：  
  最小化$\epsilon^2=\sum_{i=1}^q||\hat{x_i}-a_i\phi||^2=\sum_{i=1}^q||\hat{x_i}||^2-q\phi^TS^t\phi$  
  其中$S^T$是协方差矩阵，$\phi$是投影向量。  
  这里中间的推导式子还有点麻烦跳过了，不过化出来还算简洁，再对比看下协方差矩阵$S^t$的表达式，然后发现最小化投影后误差的问题，其实可以转化为在约束$||\phi||^2=1$下最大化关于协方差矩阵的式子$\phi^TS^t\phi$，写拉格朗日乘数法的式子（略过），最后求解出来得到的式子发现正好是$S^t\phi=\lambda\phi$。  
  这个形式就很眼熟了，其实就是求解协方差矩阵特征值特征向量的问题。  
  有了数据分布计算协方差矩阵当然是没问题的。2x2的小向量为例，是4维，然后做协方差矩阵是4x4的矩阵。最后特征值分解，求协方差矩阵的特征值和特征向量，4x4矩阵就有4个特征值和对应的特征向量。这里的特征向量其实就是坐标轴的方向，特征值就代表的这里的数据分量大小。特征向量4x1，可以化回2x2的矩阵，那这些特征向量对应小矩阵就是KLT变换的basis fuctions/images。真神奇  
  做了KLT变换之后就可以做Low-Rank Modeling（低阶建模？）去压缩了，把特征值从大到小排序，后面的就是不那么重要的维度，可以根据情况去依次移除后面的维度。  
  ###### 1 图像分块并展开为向量  
  第一步先做图像分块，块的大小决定了KLT分解的维数。例如最简单搞成一堆1\*2的小向量(x1,x2)（方便可视化，高维就不直观不好理解了，实际操作中还是做的正方形小块，即N\*N=N^2维数的一个块），那每个小向量其实可以用空间中的一个点来表示，二维的可以在直角坐标系中把这些点画出来，可能是一片接近椭圆形的区域。实际的N\*N维需要展开成一个一维向量有N\*N个元素。我们以2\*2为例说明，就是展开为1\*4的一个向量。  
  ###### 2 图像块向量拼接  
  第二步把这些小向量连起来做成一个大向量。一般图像大小可以整除小块的大小，例如6\*6的图像分为2\*2的小块，一共分9块，每块展开乘一个4个元素的向量$x_i$，合起来就是9\*4的一个大向量。  
  ###### 3 中心化找到新的中点  
  第三步做zero-mean centering，也就是把这个区域的中心点（均值点，也就是所有向量平均下来的一个向量）移动到坐标系零点，每个点都减去这个中心点坐标即可。所以先所有小向量求均值得到新的中心点。记为$\bar{x}$  
  ###### 4 中心化重建坐标系  
  记每个点的新坐标为${x_i}'$，则${x_i}'=x_{i}-\bar{x}$，新坐标系下随机变量记为$x'$  
  ###### 5 计算协方差矩阵  
  现在希望获得$x'$的协方差矩阵。$C=E[x'{x'}^T]=E[(x-\bar{x})(x-\bar{x})^T]=\frac{1}{n}\sum_{i=1}^n(x_i-\bar{x})(x_i-\bar{x})^T$（实际中全部计算太多了，毕竟那么多小向量，所以可能要做采样的协方差矩阵来作为估计值而不是准确值）（还有个协方差矩阵估计按说是除n-1的问题，忽略吧……）  
  ###### 6 求解协方差矩阵特征值特征向量  
  这里具体不赘述了，最后得到的特征向量就是新的坐标轴，特征值就是数据在这个坐标轴上的分量大小。把特征向量整理成矩阵形式，就是KLT变换的basis functions  
  #### SVD奇异值分解具体实现  
  线性代数信号处理必备工具  
  将图像视为一个矩阵A，可以分解为$A=U \Sigma V^T$。其中U是个mxm的正交矩阵，其列向量是AA^T的特征向量（可以想下mxn的乘nxm得到mxm的），要做正则化使得模长为1（特征向量原始值除以根号下各个维度的取值），顺序是按照特征值降序的顺序排列；V是个nxn的正交矩阵，其列向量是A^TA的特征向量，也是正则化模长，顺序也是特征值降序排列。（正交矩阵定义是UU^T=U^TU=I，也就是说其转置等于逆矩阵）。$\Sigma$是个mxn的对角矩阵，对角线值σ_i=\sqrt{λ^i}是等于特征值依次开根号，总共有r个，λ是AA^T和A^TA的特征值（一样的，虽然个数不同，但前几个是相同的，多的后面的为0，下面解释）  
          所以有$Av_i=σ_iu_i$，$A^Tu_i=σ_iv_i$。这个也挺好证的，把A按式子分解，正定矩阵会被消掉，选中其中U/V第i列的列向量，就是$\Sigma$中的第i个特征值乘以第i个U/V中的特征向量。  
          手算SVD。计算过程首先求$AA^T$或者$A^TA$，考虑下维度那个少算哪个（一般出题2x2的一元二次方程还好算，3x3的就不好算了）。然后求特征值&特征向量得到U/V，然后其实由于U和V有一样的特征值，所以可以直接减少另一边解特征值&特征向量的计算（特征向量可以利用上面搞单列向量的式子求）。但这里还是不能解决全部问题，毕竟另一边的维数更多，所以还有其他特征值和特征向量要求解。得到$U \Sigma V^T$三个矩阵就搞定了。  
  补充：关于矩阵的秩。这里的计算我一开始很不理解，为什么那个维度小算哪个，然后U和V特征值一样，明明一个mxm矩阵一个nxn矩阵，特征值个数都不一样。后面了解到有一个很关键的结论，就是A矩阵的秩就决定了$U \Sigma V^T$这三个矩阵的秩，矩阵的秩也对应了非零特征值个数，对应了$\Sigma$矩阵对角线上的奇异值个数。所以U和V哪个维度小算哪个，另一个就是同样的特征值加上几个零特征值（按说零特征值对应的特征向量都不需要求解，反正乘上$\Sigma$也是得0）  
  利用SVD的结果，写成一列列加和的形式就是A=\sum_{i=1}^r\sigma_iu_iv_i^T。不用全部r维，只取前p维，即$\hat{A}=\sum_{i=1}^p\sigma_iu_iv_i^T$，就是舍弃了不那么重要的维度数据实现压缩，和KLT、DCT啥的想法很像。  
          这样100x100的图像就是100x100的A，去做分解，保留前80维，就实现了压缩。  
    
  ## 视频结构  
  具体找协议图吧，大致从上到下分为  
  Sequence、GOP、帧、slice、宏块、block  
    
  ### IBP帧类型  
  I是Intra-coded，就是完整的一个图像自己编码不需要依赖其他信息，处理时用JPEG的算法即可，压缩比最低（当然H.264开始也引入了帧内预测）；P是predictive-coded，编码依赖前一个I/P帧的运动估计&补偿；B是bidirectional-coded，编码依赖前后的I/P帧，因此也压缩比最高。  
  调整GOP的格式可以通过调整N值M值来实现，N值表示Size of GOP，是两个I帧之间的距离，常见十几帧的样子；M是两个anchor帧的距离（几帧），即I/P帧之间的距离，常见1-3帧的样子。用户可以在编码器自定义N和M的值，这个是编码性能和错误传播（error propagation）的trade off，拉长N和M压缩比更高，但是存在的误差就会一直传播很久，因为Reference已经错了。  
  display顺序和编码传输顺序不一样，编码传输先搞I、P，播放按时间。编码的顺序是该编I帧P帧直接编码，该编B帧不动先记录下来，到下一个参考帧再把前面标记的B帧都编码，同时pts直接打，dts和编码的顺序一致。假设采集视频帧顺序标记为IBBP，那编码顺序是IPBB，存储和传输的顺序也是IPBB。解码侧一般也是按dts排列然后依次解码，到播放器播放的时候再按pts排出顺序。  
  典型的一些压缩比数据：I帧10:1-20:1，P帧20:1-30:1，B帧30:1-50:1  
    
  ## 传统图像视频压缩方法  
  ### 基于块的混合编码  
  基于块的混合编码框架 Hybrid Coding Framework，也可以称为基于波形的编码，视频的复杂一些，以H.264视频编码为例：  
  （插图）  
  #### 帧内预测  
  也就是单个图片压缩，去除空域冗余。  
  很基本的一个压缩思路就是记录图像数据原始的RGB值会比较大，但是如果能根据相邻像素的情况进行预测，然后只需要记录预测值和实际值之间的差值，那这个值一般而言就会比较小，进行压缩能够大大降低数据量。编码端：原始值-预测值=残差值；解码端：残差值+预测值=原始值。  
  把图像分成很多小块处理，预测的时候也是可以选择不同的block size，4x4或者16x16（8x8似乎不行），4x4块提供了9种预测模式，16x16块提供了4种预测模式。以4x4块为例，我们想办法用左边/上面的邻近像素点来预测这个块里的点，看下图会比较清楚这9种模式。做的时候就是记录模式序号，然后计算残差，选择模式的时候也是搜索残差最小的（简单的做法就是sum矩阵里所有数的绝对值）。  
  TODO：搞个图  
  #### 帧间预测  
  利用前后帧进行预测，能匹配对应的块就只需要记录少量的残差值+一个运动矢量了，去除时域冗余。即运动估计和运动补偿。  
  直接用后帧减去前帧，因为存在运动的原因，明明是类似的像素点，但是会有比较大的差距，无法直接“对齐”。这里需要做运动估计，针对某一个block，在历史帧的对应位置周边范围内搜索对应的块，找到最优匹配的块，就可以确定这个块运动的方向（即运动矢量MV motion vector）。有了运动矢量就可以做运动补偿motion compensation，实际需要记录的是当前块减去最优匹配块的残差块&MV，一般而言差值会很小。这个最优匹配块的搜索过程是需要技巧的，完全搜索一遍运算量很大。  
  ##### 补充：运动估计搜索细节  
  运动估计搜索的时候一般还是设定一个search window，例如水平和竖直搜索区域是[-p,p]，那整个窗口大小是(2p+1)x(2p+1)。做搜索的时候，距离可以用MAD（Mean Absolute Defference）表示，即宏块每个点都和加上运动矢量(i,j)之后的对应点做差取绝对值，做总和然后除以像素点数。最后目标是在search window中找到和当前宏块MAD最小的。看了个对应动画片视频的可视化的运动矢量，有点意思。  
  运动估计常用搜索方法是：  
  * full search遍历  
  准确一定能达到最优压缩比，但是慢计算量大  
  * Three step search，16x16的块，看做4x4块的组合，找到中间3x3的9个交点第一步搜索，找到最优的点，在周围4个2x2块再找到9个交点，做第二步搜索，找到其中最优的，在周围9个点第三步搜索，速度会快，但是准确率低；  
  * 2D Logarithm Search  
  每一步找5个点画个十字，如果最优的不是中心点，就以最优点为中心再画十字，如果是最优点，就在搜索围绕的8个点找到最优。这个复杂度是Log还是要思考下，有点抽象，这算是个full step与3 step之间比较好的折中，速度比3 step要慢（TODO：具体分析下），但准确率比3 step要高。  
  * Hierarchical Search，16x16认为是Level0，可以降采样两次变,8x8Level1和4x4的Level2，最开始先从Level2做运动估计，然后利用Level2的运动估计结果去在Level1中搜索，再用Level1结果指导Level0结果，这样做的优势是search window可以减少，就可以减少计算量（我一开始理解错了，搜索过程宏块大小也会跟着降采样）。  
  #### 变换  
  运动补偿后像素值就从绝对值变为残差值，会少很多。  
  从像素域转为频域，让能量集中在低频部分，区分出不重要的高频细节部分可以量化去除也不太影响整体质量  
  基本都是用DCT离散余弦变换速度快。H.264中用的是Integer变换，其实就是DCT取整了。DCT使用浮点运算，就会在变换和逆变换的时候有进位误差的问题。并且误差累积会导致较大的误差。Integer Transform就没有这个问题。提供了量化机制使用非线性的步长实现准确的码率控制。看下具体计算，这其实是从DCT变换来的，回顾DCT变化那可以用$TfT^T$矩阵相乘的方式来实现，那现在把n=4的时候的T变换矩阵乘一个系数α再进行round搞成都是整数值，最后就得到了Integer变换的H矩阵，做$HfH^T$就可以了，常用的例如α=2.5。这就是一个更“离散化”的DCT吧。H矩阵是orthogonal正交矩阵，所以xxx。逆矩阵写出来也不是很复杂。最后量化的结果$\hat{F}=rount[(HfH^T) \cdot M_f/2^{15}]$，M_f是从量化矩阵m和QP（量化参数？看起来是有QP代表了量化损失的档位，越大损失越多，所以只要调整QP就可以控制码率）来的，老师列了下对应的表格，所以M_f对着查表就行。TODO：这个公式我有一点不太理解。老师说这里不用太记公式，如果要考的话会给公式。然后逆过来dequantization的时候，公式是$\hat{f}=round[(H^{-1} (\hat{F} \cdot V_i) {H^{-1}}^T)/2^6]$  
    
  #### 量化  
  有损地近似，一段区间内的值都用一个值表示，去除一些信息量的冗余，也是压缩导致失真的原因。可以调整QP量化步长（一般记录为一个矩阵，放到Header里传输）来调整压缩的程度。  
  #### 熵编码  
  高概率的符号较短的码字，低概率的符号较长的码字，使得平均码长最短，去除统计的冗余。  
  也就是信息论中的信源编码，香农极限指示了最小码长的极限。经典常用的就是霍夫曼Huffman编码和算术Arithmetic编码，另外15年有个ANS编码（Asymmetric Numeral System，非对称数系），计算复杂度接近Huffman，性能又接近Arithmetic，所以是Sota应用也很多了。（对比信道编码是增加冗余信息，考虑到信道传输引入噪声，要在接收方纠错）  
  264和265中都是用CABAC（基于上下文的自适应二进制算术编码Context-Based Adaptive Binary Arithmetic Coding），是一种改进的算术编码，考虑了符号概率波动，不是固定按全局的静态概率分布，而是一段区间内的上下文。264标准还另有一个上下文自适应可变长编码CAVLC，不过实际效果不如CABAC，265中就淘汰了。  
  #### 环路滤波  
  因为视频编码是基于块处理的，解码后的图像块的边界就会主观看起来不对劲。最开始MPEG-2的处理方法是接收端解码重建后进行滤波使边界平滑一些。  
  ##### 去方块滤波DBF (DBF, deblocking filter)  
  TODO H.263开始就用了更高级的方式，编码端进行变换+量化后，直接做反量化+反变换  
  ##### 样点自适应补偿 (SAO, sample adaptive offset)  
  H.265增加的一个补偿  
    
  ## 视频编码标准发展  
  视频编码标准组织：ISO+IEC组成了MPEG发MPEG标准，ITU发H.261/H.263标准，后面它们决定合作，ITU和ISO/IEC一起发了H.262（所以又叫MPEG-2 part2）/H.264（又叫做MPEG-4 part10）/H.265/H.266。  
  实际上这些标准是相似的，沿着同一个框架不断补充优化细节，并没有巨大的差异。所以一般从MPEG-1开始认真理清细节。下面个标准概述  
  * MPEG-1:1993年，为CD-ROMs的视频音频存储设计，只支持逐行扫描，只支持420、352x288分辨率，IPB帧。  
  * MPEG-2:1995年，为DVD设计，支持420 422，支持隔行和逐行扫描，引入了不同profiles和levels是比较特别的，也支持了scalable coding多层的编码  
  * MPEG-4 Part2:1999年，支持移动平台的低清视频应用，支持object-based或者content-based coding（可以分前景背景目标的编码很神奇，但是因为这样效率不高，实际是没有咋用的）  
  * MPEG-4 Part10（或者说H.264 AVC）:2003年，搞了不少优化，也是实际应用最广的编码，使用integer transform（不用DCT了），变块size的运动补偿，directional spatial的帧内预测（宏块之间可以互相预测），多参考帧，in-loop deblocking filter（去区块滤波器）等  
    
  换成ITU的顺序看看  
  * H.261:1998年，为视频会议设计（重视低码率实时性），支持CIF和QCIF分辨率（说非常低？）420，也是基于块的混合编码，integer pixel的运动补偿（这个我没懂，说后面都是可以支持半像素/四分之一像素的运动补偿）。  
  * H.262:1995，相比H.261，支持了B帧和隔行扫描  
  * H.263: 1996nian ,plus版本1998年，提升了视频会议质量  
  * H.264: 讲过了，2003年  
  * H.265:2013年，支持高分辨率视频，8k 120帧，更强的预测模式和变换块大小  
  * VP8: 谷歌开源，支持420 8bit 逐行扫描 4k，运动向量可以做到1/4像素luma和1/8像素chroma，自适应去区块滤波器  
  * VP9: 支持8k 120帧，使用superblock（32*32 64*64）来适配高清视频中的更大区域，增强了运动补偿的插值可以做到1/8像素（啥8-tap filter，有8个系数？）  
  * VP10  
  * AV1  
    
  ### H.264补充细节  
  #### 关于句法元素  
  说的有点高级，其实就是数据单元吧。从大到小依次是视频序列-单帧图像-图像分片-宏块（16x16 YUV数据）-子块  
  #### 关于系统分层  
  为了让传输和底层编码算法功能解耦，分了两层。  
  视频编码层VCL，做视频编码，输出SODB string of data bits编码后原始数据。  
  网络适配层NAL，封装SODB为适合网络传输的含有首部的整byte数据流。先给SODB加结尾比特10000……字节对齐变成RBSP raw byte sequence payload，然后每一个片分一段，添加NALU Header分成一个个NALU（所以一个NALU不是一帧也不是一个宏块）。除了封装码流的NALU存的是一个片Slice，还定义了存序列参数集SPS（序列全局参数用于初始化解码器）、图像参数集PPS（对应具体单帧图像的参数用于解码）、补充增强信息SEI（就是插广告、p2p分片等嘛）的NALU类型。  
  再上层的“封装”对应RTMP、HTTP-FLV、HLS这些传输协议了，就是怎么传NALU的事情，不涉及转码了。  
  #### H.264和H.265区别  
  效果上说H.265平均比H.264同质量降低50%码率，并且最高支持到8k 120fps格式。  
  ##### 灵活的块大小  
  宏块16x16升为64x64。  
  区分了编码单元CU、预测单元PU、变换单元TU。不同环节分开设置单元大小很灵活。  
  编码单元是区分编码处理方式（帧间预测/帧内预测/跳过）的，同一个编码单元内编码处理方式一致，可以从8x8 16x16 32x32 64x64变化，用四分树quadtree数据结构组织，边缘纹理就分的细一点，色块分的大一点，性能更好。  
  预测单元是在一个编码单元内细分，一个编码单元可以分为1、2、4个预测单元（固定8种分割方式）。预测单元是区分预测参数的（运动向量方向，运动向量差值）。  
  变换单元也是在编码单元内细分。最多3层，TU大小4x4 8z8 16x16 32x32。变换单元内一起做变换和量化。分的大块压缩更多，分的块小保持纹理。这里原理还可以再思考下。  
  ##### 运动补偿预测方向增加  
  支持Y亮度块35个方向，UV色度块5个方向。  
  ##### 样点自适应补偿SAO滤波  
  在去方块滤波后增加了一个补偿步骤。这个补偿值是直接把重建帧和压缩前原始帧比较得到的。当然也不可能无损地把所有插值都补偿了，还是大致减小失真。  
  流程大概还是先分块，然后准备了很多补偿模板（相当于是字典减少数据量，不可能去每个像素记录），分块记录下补偿类别和补偿值即可。  
  这样做虽然额外增加了处理开销和码率，但是由于减小失真也有利于预测降低残差，实际使用中一般是可以以增加一点点的计算量降低一点点的码率的。  
  ##### 升级并行处理&插帧方法  
  TODO  
  #### H.266和H.265区别  
  简单过一下了，Versatile VIdeo Coding VVC/MPEG-I Part 3。适配SDR HDR 360视频 全景视频。支持到16k，位深10-16bit。相比H.265又降50%。  
    
    
  ### 补充：基于内容的编码object-based coding  
  基于块的编码不灵活，如果一个块中含有物体边界，不同物体不同的运动方向就会导致用同一个运动矢量表示误差。如果能识别物体划分区域在分开处理就会效率更高，可以称为基于内容的编码。这个思想挺好的，MPEG4-part2 1998年就定稿了，但实际上这种方式没有得到普遍应用。但沉浸式视频中应重新考虑。  
    
  # 深度学习+视频编码  
  ## 研究路线分类  
  参考ETH博士生 杨韧的讲座  
  几个大方向  
  * 1.压缩视频质量增强 这个是不涉及到编解码框架的改动，直接处理解码后的视频  
  个人认为这个没必要单独归类  
  * 2.结合深度网络的传统视频压缩 把编码框架中的一些模块替换为深度网络，例如环路滤波器、运动补偿模块  
  个人认为即模块替换方法  
  * 3.端到端优化的视频压缩深度网络 完全替换为深度网络，已经有不少研究，可能未来会成为主流编码器技术替代DCT  
  即端到端方法  
  * 4.兼容一般播放器的深度学习压缩方法 因为深度网络需要训练，必须要对应的解码器，这里只搞编码不需要播放器有啥处理 例如搞HEVC CTU大小这里  
  个人认为即算法替换  
    
  所以我重新做分类认为以下会比较清晰  
  * 算法替换Algorithm Replacement  
  一些尽量影响小可以直接使用的，不涉及到编解码框架和模块的改动，只是使用传统框架中的算法，兼容当前应用，只是将深度学习应用于当前模型的一些具体算法当中。  
  例如压缩视频质量增强直接处理解码后的视频、基于深度学习做多尺度的块划分（CTU分割）  
  * 模块替换Module Replacement  
  理论上传统视频编码的各个部分都可以使用深度神经网络进行替换，包括预处理、帧内/帧间预测（运动补偿）、变换、量化、熵编码、环路滤波等。其中研究最多的是基于深度学习的帧内预测、帧间预测以及环路滤波。  
  * 端到端End-to-End  
  端到端的视频编码框架严格而言并没有完全脱离经典的混合编码框架，并不是仅仅依靠单独的神经网络就完成了全部的编码、解码流程。这是因为视频压缩仍然是一个复杂多步骤的系统任务。但是网络中的核心模块已经完全替换为深度神经网络，不再进行传统的基于变换的帧内压缩和基于最佳匹配块搜索的帧间压缩。  
  ### 端到端视频压缩分类  
  #### 基于预测编码  
  #### 基于条件编码  
  #### 基于生成编码  
    
    
  ## 深度学习+图像/视频编码问题  
  量化器不可微，因为量化器的输入是连续值输出是离散值，输出对于输入的微小变化不敏感，这样就到导致没法计算微分，因为△x变了但是△y没变。对于深度学习而言就很难做反向传播，因为需要求导计算梯度，得不到梯度就没法优化调参了。这里我还疑惑了深度学习中没有其他不可微的问题吗？图像识别任务中输入的图像数据本身不就是量化过的吗？这里应该是通过卷积这种加权平均的操作就转变为了连续的输出值，所以就可以求导。当然也有一些没那么好求导的情况，例如池化层直接多输入对应一个输出了也是不可导的，但是可以解决，最大池化就记录下下采样的最大值是哪个位置，反向传播回对应的位置就可以，一对一还是可以求导；平均池化更简单，直接反向传播也是把梯度值除以N平均传递回多个输入。  
  Learning Convolutional Networks for Content-weighted Image Compression（二值化）文章中提到传统图像压缩有离散熵估计的问题用于码率控制，在深度学习中不好做。我理解就是为了熵编码要先知道每种符号的出现概率？对于图像而言可以遍历统计每个符号的出现概率，不好全做的话就是采样来估计，或者使用概率模型来估计（例如用GMM或者马尔科夫链模型HMM）。但是看文章似乎是分配码率的样子。  
    
    
    
  # 论文整理  
  整体论文数量还是有一些，感觉不算特别多    
  ## End-to-end Optimized Image Compression  
  2016年挂ArXiv 2017年 ICLR 奠基论文，Google的Johannes Balle大佬，确定了深度学习图像压缩方法的基本模型，后续的研究基本遵循了这个框架在改进。    
  ### 模型框架  
  分了三个部分：analysis transform分析变换（编码器），uniform quantizer均匀量化器（量化器），synthesis transform合成变换（解码器）    
  * 分析变换重复了三段卷积线性滤波器+非线性激活函数，那就是实现卷积神经网络特征提取从而实现压缩。还提出了特别适合图像重建问题的归一化层GDN Generalized Divisive Normalization，不像是BN层会引入噪声。（TODO：GDN层原理）    
  * 均匀量化器是输出的连续值为了方便存储传输要化有限整数，但是就会因为不可导造成无法反向传播。所以这里引入了代理函数，训练过程中不进行量化，而是使用均匀噪声来模拟量化造成的误差（TODO：有推导不太理解），就可以求导。    
  * 合成变换，基本就是分析变换网络结构倒转过来恢复实现解码，GDN层变成对应的IGDN层，降采样变成升采样。    
    
  ## Lossy image compression with compressive autoencoders  
  2017年 ICLR经典论文  
    
  ## Variational image compression with a scale hyperprior  
  2018年 还是Google的Johannes Balle大佬的改进论文，引入了深度神经网络估计概率分布模型  
    
  ## Joint Autoregressive and Hierarchical Priors for Learned Image Compression  
  2018年 NeurlPS 还是Google的Johannes Balle大佬的改进论文，又搞得比较复杂  
    
  ## Learning Convolutional Networks for Content-weighted Image Compression  
  2018年 CVPR经典论文    
  香港理工大学 Li Mu    
  ### 问题：    
  用CNN做端到端图像压缩，存在的困难有    
  * 1.量化器是不可导的，就没法做反向传播；    
  * 2.需要离散熵估计来做速率控制（这个离散熵估计的概念没有找到准确出处和含义，应该是统计每个符号出现的频率作为概率计算信息熵再离散化，得到的和当做码率的大小），这一步也不可导    
  ### 方法：    
  * 1.图像不同部分重要性不同，所以比特率分配也应该不同，引入importance map来替代离散熵估计从而解决问题2（TODO：为什么重要性图就可以代替熵估计，通过权重控制比特率分配是怎么实现的？）    
  看网络结构importance map的实现是一个旁路的分支结构，编码器的输出分了一路输入重要性图网络得到一个包含每一个像素点重要权重的图，这个图量化后作为一个mask掩码表去乘二值化的输出，实现所谓的内容重要性加权。    
  * 2.用二值化对编码器的输出进行量化，反向传播中再引入二进制运算的代理函数，就可导从而可以反向传播了（TODO：这里可以参考BNN 二值神经网络，能极大减少计算量和内存开销，但这里量化原理没看懂得专门研究下，直观感觉直接把很高维的提取特征用二值化表示还能保持一定的效果很不可思议），解决了问题1。    
  ### 效果：  
  在低比特率的图像压缩中，SSIM优于JPEG、JPEG 2000，边缘更清晰、纹理更丰富、伪影更少。    
  ### 想法：  
  给我感觉用深度学习解决一些本不适用深度学习的任务时，关键是解决任务流程中个别步骤不可导的问题，引入不同的可导方法来近似实现原步骤效果并支持反向传播跑通。所以就要理解替代方法为什么可以近似原步骤效果？为什么可以求导？    
    
  ## End-to-End Optimized ROI Image Compression  
  2020年TIP经典 ROI图像压缩论文    
  不同于之前传统的ROI图像压缩方法是分两步先做ROI获得ROI区域作为Mask之后再送入编码器处理，直观好理解但是没法放在一起优化，这篇文章提出了把ROI和encoder做在一起的框架。（TODO：这里我还不是很理解为什么做在一起就可以更好的优化，是指ROI区域预测结果也是动态调整的吗？）    
  ### 网络结构：  
  看网络结构应该和Learning Convolutional Networks for Content-weighted Image Compression文章中的importance map的做法类似，是encoder的后半部分输出分两路，一路正常出多尺度representation特征，一路输入旁路分支预测出ROI掩码图（二值的），然后组合在一起（说是element filtering operation不知道是不是就加权乘）。这里说得到的是implicit ROI Mask（TODO：隐式是什么意思？）。    
  使用ROI做码率分配的实现原理是在encoder中加入了MSD多尺度分解块来实现的，decoder对应用IMSD（TODO：MSD是另一篇文章提出的可以看下原理）。    
  ### 想法：  
  我理解做ROI图像压缩就是想办法把生成ROI掩码图的网络结构加入网络，实现图像不同部分对Loss的影响不同就可以实现多保留主体牺牲背景的效果。    
    
  ## DVC An end-to-end deep video compression framework  
  这是首个End-to-End视频压缩模型，其系统框图如图所示（插图）。  
  采用基于深度学习的光流预测来获取运动信息，使用两个自动编码器分别对预测运动信息和剩余信息进行编码。  
  可以看到框架结构其实整体和传统的视频编码框架相似，但是有两个非常大的改变。一是用于帧间压缩的运动估计和补偿，被基于CNN的光流估计网络和运动补偿网络所替代。二是传统的用于帧内压缩的变换、量化步骤以及对应的逆变换已经被基于深度学习的residual encoder network所替代。所有的模块使用同一个损失函数联合训练，设计损失函数在压缩比和质量之间取得平衡。  
  实验验证DVC在性能上能实现在保持相同视频质量的情况下，比H.264的压缩率提升40%，也就是接近H.265的成绩。这为端到端视频压缩框架的研究做了一个良好的开端。  
    
  ## Deep Contextual Video Compression  
  2021年微软亚洲研究提出的模型。没有使用预测编码，而是创新性地使用了条件编码，可以视为预测编码方式的一种拓展。  
  预测编码简单有效，利用了预测帧和当前帧之间相关性强，差值较小的特点，被广泛应用。但是预测编码只是条件编码的一种简化情况，理论上在视频压缩中，还有运动关系、图像纹理、图像颜色等等附加的条件信息都可以提升压缩比，也就是已编码的所有信息都可以为当前帧压缩提供支持。但是在传统视频编码技术中，这些条件很难得到应用，因为输入过于复杂，难以人工提取出其中有效的特征，而深度学习为这一技术提供了可能。  
  如图所示（插图），DCVC在编解码过程中通过Context Generation网络引入了额外的上下文信息。然而DCVC模型也具有严重的缺陷，由于条件编码的复杂性，给网络的训练带来了极大的困难，难以复现。但这一工作仍然为基于深度学习的视频压缩提供了新的研究思路。